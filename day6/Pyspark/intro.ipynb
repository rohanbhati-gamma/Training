{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb52fde",
   "metadata": {},
   "source": [
    "#### Spark\n",
    "Apache Spark is a lightning fast real-time processing framework.\n",
    "It does in-memory computations to analyze data in real-time.\n",
    "Apache Spark has its own cluster manager, where it can host its application.\n",
    "It uses HDFS (Hadoop Distributed File system) for storage and it can run Spark applications on YARN as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d386947",
   "metadata": {},
   "source": [
    "#### PySpark\n",
    "Using PySpark, you can work with RDDs in Python programming language also. It is because of a library called Py4j that they are able to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448e920",
   "metadata": {},
   "source": [
    "#### PySpark - SparkContext\n",
    "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.\n",
    "\n",
    "SparkContext uses Py4J to launch a JVM and creates a JavaSparkContext. By default, PySpark has SparkContext available as sc, so creating a new SparkContext won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691872f3",
   "metadata": {},
   "source": [
    "#### RDD (Resilient Distributed Dataset)\n",
    "RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster. RDDs are immutable elements, which means once you create an RDD you cannot change it. RDDs are fault tolerant as well, hence in case of any failure, they recover automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5801f588",
   "metadata": {},
   "source": [
    "Let us understand these two ways in detail.\n",
    "\n",
    "Transformation − These are the operations, which are applied on a RDD to create a new RDD. Filter, groupBy and map are the examples of transformations.\n",
    "\n",
    "Action − These are the operations that are applied on RDD, which instructs Spark to perform computation and send the result back to the driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50628f19",
   "metadata": {},
   "source": [
    "#### Broadcast & Accumulator\n",
    "For parallel processing, Apache Spark uses shared variables. A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, so that it can be used for performing tasks.\n",
    "\n",
    "Two types of share variables:\n",
    "Broadcast and Accumulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e7485",
   "metadata": {},
   "source": [
    "Broascast -\n",
    "Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks.\n",
    "\n",
    "Accumulator - \n",
    "A write-only shared variable used to accumulate values (e.g., counters or sums) from multiple tasks.\n",
    "Best for debugging or monitoring, not returning results to your program logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0549c564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 15:54:11 WARN Utils: Your hostname, my-HP-EliteDesk-800-G1-DM resolves to a loopback address: 127.0.1.1; using 192.168.29.156 instead (on interface wlp6s0)\n",
      "25/06/04 15:54:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/04 15:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Broadcast Example\")\n",
    "\n",
    "# A large read-only lookup dictionary\n",
    "lookup_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "\n",
    "# Broadcast it\n",
    "broadcast_var = sc.broadcast(lookup_dict)\n",
    "\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\n",
    "\n",
    "# Use the broadcast variable inside map\n",
    "result = rdd.map(lambda x: broadcast_var.value.get(x, 0))\n",
    "\n",
    "print(result.collect())  # Output: [1, 2, 3, 1, 2, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab92b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Accumulator Example\")\n",
    "\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "def count_even(x):\n",
    "    if x % 2 == 0:\n",
    "        accum.add(1)\n",
    "    return x\n",
    "\n",
    "# Perform an action\n",
    "rdd.map(count_even).collect()\n",
    "\n",
    "print(\"Number of even numbers:\", accum.value)  # Output: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656779e",
   "metadata": {},
   "source": [
    "#### SparkConf\n",
    "To run a spark application we need to set few configurations and paramaters.\n",
    "\n",
    "Initially, we will create a SparkConf object with SparkConf()\n",
    "\n",
    "\n",
    "set(key, value) − To set a configuration property.\n",
    "\n",
    "setMaster(value) − To set the master URL.\n",
    "\n",
    "setAppName(value) − To set an application name.\n",
    "\n",
    "get(key, defaultValue=None) − To get a configuration value of a key.\n",
    "\n",
    "setSparkHome(value) − To set Spark installation path on worker nodes.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737e9d6",
   "metadata": {},
   "source": [
    "#### Spark Files\n",
    "SparkContext.addFile(path) - Distributes a file (local or remote) to all worker nodes so each task can access it locally.\n",
    "\n",
    "SparkFiles.get(\"filename\") - Returns the local path to the file that was distributed via addFile\n",
    "\n",
    "SparkFiles.getRootDirectory() - Returns the directory where Spark stores files added via addFile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b823753",
   "metadata": {},
   "source": [
    "#### Storage Level\n",
    "StorageLevel decides how RDD should be stored. In Apache Spark, StorageLevel decides whether RDD should be stored in the memory or should it be stored over the disk, or both. It also decides whether to serialize RDD and whether to replicate RDD partitions.\n",
    "\n",
    "class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1b56d",
   "metadata": {},
   "source": [
    "#### MLlib\n",
    "Apache Spark offers a Machine Learning API called MLlib. PySpark has this machine learning API in Python as well. It supports different kind of algorithms, which are mentioned below −\n",
    "\n",
    "mllib.classification − The spark.mllib package supports various methods for binary classification, multiclass classification and regression analysis. Some of the most popular algorithms in classification are Random Forest, Naive Bayes, Decision Tree, etc.\n",
    "\n",
    "mllib.clustering − Clustering is an unsupervised learning problem, whereby you aim to group subsets of entities with one another based on some notion of similarity.\n",
    "\n",
    "mllib.fpm − Frequent pattern matching is mining frequent items, itemsets, subsequences or other substructures that are usually among the first steps to analyze a large-scale dataset. This has been an active research topic in data mining for years.\n",
    "\n",
    "mllib.linalg − MLlib utilities for linear algebra.\n",
    "\n",
    "mllib.recommendation − Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user item association matrix.\n",
    "\n",
    "spark.mllib − It currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.mllib uses the Alternating Least Squares (ALS) algorithm to learn these latent factors.\n",
    "\n",
    "mllib.regression − Linear regression belongs to the family of regression algorithms. The goal of regression is to find relationships and dependencies between variables. The interface for working with linear regression models and model summaries is similar to the logistic regression case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f08a34",
   "metadata": {},
   "source": [
    "#### Serializer\n",
    "PySpark uses serializers to convert Python objects into a byte stream for efficient storage or transmission across a network. Deserialization does the reverse, transforming the byte stream back into a usable object. \n",
    "\n",
    "1. Default Serializer:\n",
    "Java Serializer:\n",
    "This is the default serializer in PySpark. It handles Spark's internal objects, such as RDDs and DataFrame metadata. It is robust but can be slower for Python-heavy workloads.\n",
    "\n",
    "Pickle Serializer:\n",
    "Uses Python's pickle library. It can serialize almost any Python object, making it versatile. However, it may not be as fast as more specialized serializers. \n",
    "\n",
    "\n",
    "2. Other Serializers:\n",
    "Marshal Serializer: Faster than PickleSerializer but supports fewer data types.\n",
    "\n",
    "Kryo Serializer: A more efficient serializer that can provide better performance, especially for network-intensive applications. However, it requires custom registration.\n",
    "\n",
    "\n",
    "3. Batched Serializers:\n",
    "AutoBatchedSerializer: Dynamically chooses the batch size from the input object.\n",
    "\n",
    "FlattenedValuesSerializer: Used for streams of list of pairs, creating lists of the same size for each key. It is involved in shuffle operations.\n",
    "\n",
    "##### Working of Serializer\n",
    "\n",
    "Step 1: Data is prepared on the driver node and serialized into a binary format by the configured serializer.\n",
    "\n",
    "Step 2: Serialized data is transmitted to worker nodes in the cluster.\n",
    "\n",
    "Step 3: Worker nodes deserialize the data for processing.\n",
    "\n",
    "Step 4: The results from worker nodes are serialized again before being sent back to the driver node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb2c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/04 17:12:10 WARN Utils: Your hostname, my-HP-EliteDesk-800-G1-DM resolves to a loopback address: 127.0.1.1; using 192.168.29.156 instead (on interface wlp6s0)\n",
      "25/06/04 17:12:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/04 17:12:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulated value is -> 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext(\"local\", \"Accumulator app\") \n",
    "num = sc.accumulator(10) \n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num \n",
    "print(\"Accumulated value is ->\",final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b74c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab3954a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n",
      "<class 'pyspark.accumulators.Accumulator'>\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "num = sc.accumulator(10) \n",
    "# print(type(num))\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([20,30,40,50]) \n",
    "print(type(rdd.foreach(f))) \n",
    "# print(\"Accumulated value is ->\",num)\n",
    "\n",
    "\n",
    "print(type(num))\n",
    "\n",
    "print(type(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc95976",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
