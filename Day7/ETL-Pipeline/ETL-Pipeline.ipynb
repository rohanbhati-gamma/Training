{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d432a081",
   "metadata": {},
   "source": [
    "### From API to Dashboard: Building an End-to-End ETL Pipeline with Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd63647",
   "metadata": {},
   "source": [
    "### Project flow:\n",
    "\n",
    "1. Data Factory hourly schedule\n",
    "\n",
    "2. Run data extraction code\n",
    "\n",
    "3. Store raw data\n",
    "\n",
    "4. Transform raw data\n",
    "\n",
    "5. Store transformed data\n",
    "\n",
    "6. Visualize transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc61ad",
   "metadata": {},
   "source": [
    "#### Used Services\n",
    "In this project, we will be using the following services:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32836856",
   "metadata": {},
   "source": [
    "Azure Cloud — Microsoft’s cloud computing platform that provides a broad range of services, including computing, storage, networking, and analytics. In this project, Azure serves as the foundation for deploying and managing the entire ETL pipeline.\n",
    "\n",
    "Azure Data Factory — A cloud-based data integration service that allows the creation, scheduling, and orchestration of ETL workflows. In this project, it orchestrates the entire data pipeline.\n",
    "\n",
    "Azure Function — A serverless compute service that enables event-driven execution of code. In this project, it is used to extract raw job market data from the Adzuna API, process it, and send it to Azure Data Lake Storage.\n",
    "\n",
    "Data Factory Trigger — A feature in Azure Data Factory that automates the execution of pipelines based on schedules or events. In this project, it ensures that the ETL pipeline runs hourly to continuously ingest fresh job market data.\n",
    "\n",
    "Azure Databricks — A cloud-based analytics platform optimized for big data and AI workloads, built on Apache Spark. In this project, it is used to transform raw extracted data and save it into Delta Lake.\n",
    "\n",
    "Delta Lake — An open-source storage layer that brings ACID transactions, schema enforcement, and time-travel capabilities to big data workloads. In this project, it ensures data consistency and reliability when storing transformed job data.\n",
    "\n",
    "Delta Lake Table — A managed table format in Delta Lake that provides structured, scalable storage with built-in support for versioning and incremental updates. In this project, it stores transformed job market data for efficient querying and analysis using Power BI.\n",
    "\n",
    "Azure Data Lake Storage Gen2 — A scalable and secure cloud storage service optimized for big data analytics. In this project, it acts as the staging area for raw and processed data before transformation and loading into Delta Lake.\n",
    "\n",
    "Azure Key Vault — A cloud service for securely storing and managing secrets, such as API keys, database credentials, and certificates. In this project, it stores connection credentials and other sensitive configurations to ensure security.\n",
    "\n",
    "Terraform — An infrastructure-as-code (IaC) tool that enables automated provisioning and management of cloud resources. In this project, it is used to deploy and manage Azure resources for the ETL pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd947e",
   "metadata": {},
   "source": [
    "#### Data Source\n",
    "In this project, we query the Adzuna API to extract job listings specifically related to Data Engineer roles in Canada. The API is configured to get job posts in Canada that mention data engineer in the job title or description. Additionally, we limit the results to job listings posted within the last two days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f2bd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
